# Generative-Ai-Image-Analytics

A simple Streamlit demo that lets a user upload an image, ask a natural-language question about it, and get a text response generated by Google Gemini (via the `google.generativeai` SDK).

> **Note:** this is a demo. Do not include your API key in source control. Store it in a `.env` file and add `.env` to `.gitignore`.

---

## Project files

* `app.py` — Streamlit application (see local file: `/mnt/data/app.py`).
* `.env` — (not committed) contains `GEMINI_API_KEY`.
* `requirements.txt` — dependencies.

---

## What it does (human summary)

This app provides a minimal user interface: upload an image, type a question about the image (for example: "Describe objects in the image", "What abnormalities do you see?", or "Count the number of people"), and click **RESPONSE**. The app sends your prompt and the image to Google Gemini and displays the model's textual reply.

It's a quick way to experiment with multimodal capabilities and prompt design without building full backend infrastructure.

---

## How to run (local)

1. Create a virtual environment and activate it:

```bash
python -m venv .venv
source .venv/bin/activate   # macOS / Linux
.\.venv\Scripts\activate  # Windows (PowerShell)
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Add your Gemini API key to a `.env` file in the project root:

```
GEMINI_API_KEY=sk-....
```

4. Run the app:

```bash
streamlit run app.py
```

Open the URL shown in your terminal (usually `http://localhost:8501`).

---

## Key implementation notes

* The app reads the API key with `python-dotenv` and calls `genai.configure(api_key=...)`.
* It uses `st.file_uploader` to accept image files and `PIL.Image.open()` to preview them.
* The code calls Gemini using `genai.GenerativeModel("gemini-2.0-flash")` and attempts to pass the prompt and the `PIL.Image` object together. Depending on your installed SDK version, the exact call and response fields may differ — inspect the response object during development.
* Always test with small prompts first and watch your API usage (calls to Gemini are billable).

---

## Troubleshooting

* **No text returned / SDK error:** check the installed `google-generativeai` version and inspect the `response` object (`dir(response)` and `print(response)`) to find the attribute that contains text. The SDK can change across releases.
* **API key errors:** make sure `GEMINI_API_KEY` is set in your environment and not committed.
* **Image format issues:** convert images to RGB (`img.convert('RGB')`) before sending if needed.

---

## Security & ethics

* Do not upload private or sensitive images to a demo connected to a third-party LLM.
* Avoid sending personal data in prompts.
* Be transparent in any demo about limitations and possible model hallucinations.

---

## Resume-friendly description (two options)

**Short:**

> Built a Streamlit app that integrates Google Gemini for multimodal image analysis. Implemented file upload, prompt-driven model queries, image preview, and basic response rendering.

**Detailed (1-line bullet for experience section):**

> Developed `Generative AI Image Analyzer` (Streamlit, Google Gemini, Pillow). Implemented a multimodal input pipeline (image upload + prompt), secure API-key handling with `.env`, and robust error handling for model interactions.

---

## Where the main code lives

Open the Streamlit app source: `/mnt/data/app.py` (local path).
